project:
  save_dir: './saved_checkpoints/stage1_from_config/'
  use_wandb: true
  wandb_project: 'mantis-stage1'
  wandb_run_name: 'stage1-c96-frankensplit-152k'
  wandb_tags: ['stage1', 'c96', 'frankensplit', '152k-params']

data:
  data_dir: '~/imagenet-1k-wds'
  num_workers: 1
  prefetch_factor: 2
  image_size: 224

model:
  # Model configuration  
  type: 'mantis_stage1'
  config:
    # Client parameters (FrankenSplit architecture: 3→96→48→48 channels)
    client_params:
      stem_channels: 96      # Increased stem output (was 64)
      encoder_channels: 48   # Final encoder output
      num_tasks: 4      # Number of tasks (for client initialization)
    
    # Decoder parameters (for Stage 1 feature reconstruction)
    decoder_params:
      input_channels: 48     # VIB bottleneck channels (matches vib_channels)
      output_channels: 512   # Teacher feature dimensions (ResNet50 layer2)
    
    # VIB bottleneck
    vib_channels: 48  # Matches encoder output
  resume_checkpoint: null # Optional path to stage1 checkpoint to resume training from
  client:
    vib_channels: 48
    encoder_channels: 48  
    stem_channels: 96  # Updated to match increased architecture

training:
  device: 'cuda'
  num_epochs: 100
  batch_size: 32
  grad_accumulation_steps: 1
  grad_clip_norm: 0.5
  use_amp: true
  beta_warmup_epochs: 25  # Beta warm-up duration
  optimizer:
    lr: 0.00005
    weight_decay: 0.01
  scheduler:
    type: 'CosineAnnealingLR'
    params:
      T_max: 50 # Should match num_epochs
      eta_min: 0.0000005
  loss_weights:
    beta_stage1: 0.01
    lambda_cos: 0.3

logging_and_saving:
  log_freq: 200
  save_freq: 1
  monitor_memory: true
  profile_batches: 0 