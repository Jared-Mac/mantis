# configs/stage2_multi_dataset_training.yaml
project:
  save_dir: './saved_checkpoints/stage2_multi_dataset/'
  use_wandb: true
  wandb_project: 'mantis-stage2-multi-dataset'
  wandb_run_name: null # auto-generated if null
  wandb_tags: ['multi-dataset', 'cifar100', 'stl10', 'flowers102']

data:
  data_root: './data/multi_task'  # Root for all datasets
  use_multi_dataset: true
  download_datasets: true
  num_workers: 2
  prefetch_factor: 2
  image_size: 224

model:
  type: 'mantis_stage2'
  stage1_checkpoint_path: './saved_checkpoints/stage1_from_config/best_model.pth'
  freeze_stem: true
  resume_stage2_checkpoint: null # Optional path to stage2 checkpoint to resume from
  config:
    # --- Shared & Stage 1 Dependent Params ---
    vib_channels: 48
    stem_channels: 96
    encoder_output_channels: 48 # Final output of FiLMedEncoder, should match vib_channels

    # --- Stage 2 Specific Architecture Params ---
    num_tasks: 3 # CIFAR-100, STL-10, Flowers-102
    
    # Client-side (post-stem) parameters
    task_detector_hidden_dim: 64
    film_gen_hidden_dim: 64

    # Server-side parameters
    decoder_output_channels: 512 # To match ResNet layer2 feature dimension
    
    # These are not currently used by the refactored script but kept for reference
    encoder_blocks: 3
    encoder_architecture: 'convgdn'

training:
  device: 'cuda'
  num_epochs: 50  # Reduced for faster convergence with smaller datasets
  batch_size: 16  # Increased from 3 since datasets are smaller
  grad_accumulation_steps: 2  # Reduced to maintain reasonable effective batch size
  use_amp: true
  optimizer:
    main_lr: 0.0001
    backbone_lr: 0.00001
    weight_decay: 0.01
    gradient_clip_val: 1.0
  scheduler:
    type: 'CosineAnnealingLR'
    params:
      eta_min: 0.000001
  loss_weights:
    lambda_task: 1.0
    lambda_downstream: 1.0
    beta_prime: 0.01

logging_and_saving:
  log_freq: 50  # More frequent logging for smaller dataset
  save_freq: 2  # Save every 2 epochs
  monitor_memory: true
  profile_batches: 0 