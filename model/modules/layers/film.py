"""
This module implements Feature-wise Linear Modulation (FiLM) layers and generators.

FiLM layers (`FiLMLayer`) allow for conditioning feature maps by applying
affine transformations (scale and shift). The parameters for these
transformations (gamma and beta) are generated by a `FiLMGenerator` based on a
conditioning signal. This mechanism is used for task adaptation in models like
`TaskConditionedFiLMedAnalysisNetwork` (in `model.modules.analysis.py`) and
`FiLMedHFactorizedPriorCompressionModule` (in `model.modules.compressor.py`),
where the conditioning signal might come from a `TaskProbabilityModel` (in
`model.modules.task_predictors.py`).

Key classes provided by the module:
    - FiLMLayer: Applies the affine transformation to feature maps.
    - FiLMGenerator: Generates the gamma and beta parameters for FiLMLayer from
      a conditioning input.
"""
import torch
import torch.nn as nn

class FiLMLayer(nn.Module):
    """
    Feature-wise Linear Modulation layer.
    Applies affine transformation (scale and shift) to feature maps.
    """
    def forward(self, x: torch.Tensor, gamma: torch.Tensor, beta: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x (torch.Tensor): Input feature maps of shape (B, C, H, W) or (B, C).
            gamma (torch.Tensor): Scale parameters of shape (B, C) or (B, C, 1, 1) for 2D.
            beta (torch.Tensor): Shift parameters of shape (B, C) or (B, C, 1, 1) for 2D.

        Returns:
            torch.Tensor: Modulated feature maps.
        """
        # Ensure gamma and beta are broadcastable to x's shape
        if x.dim() == 4 and gamma.dim() == 2: # (B,C) for (B,C,H,W)
            gamma = gamma.unsqueeze(-1).unsqueeze(-1)
            beta = beta.unsqueeze(-1).unsqueeze(-1)
        elif x.dim() == 2 and gamma.dim() == 4: # (B,C,1,1) for (B,C) - less common but handle
             gamma = gamma.squeeze(-1).squeeze(-1)
             beta = beta.squeeze(-1).squeeze(-1)
        elif x.dim() != gamma.dim():
            raise ValueError(f"Input x dim {x.dim()} and gamma/beta dim {gamma.dim()} are not compatible for broadcasting.")

        return gamma * x + beta

class FiLMGenerator(nn.Module):
    """
    Generates FiLM parameters (gamma and beta) from a conditioning input.
    """
    def __init__(self, cond_dim: int, num_features: int, hidden_dim: int = None):
        """
        Args:
            cond_dim (int): Dimensionality of the conditioning input.
            num_features (int): Number of features to modulate (e.g., channels in a Conv layer).
            hidden_dim (int, optional): Dimension of the hidden layer.
                                       If None, uses cond_dim. Defaults to None.
        """
        super().__init__()
        self.num_features = num_features
        if hidden_dim is None:
            hidden_dim = cond_dim

        self.gamma_beta_generator = nn.Sequential(
            nn.Linear(cond_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 2 * num_features) # 2 for gamma and beta
        )

    def forward(self, cond_input: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            cond_input (torch.Tensor): Conditioning input of shape (B, cond_dim).

        Returns:
            tuple[torch.Tensor, torch.Tensor]:
                gamma (torch.Tensor): Scale parameters of shape (B, num_features).
                beta (torch.Tensor): Shift parameters of shape (B, num_features).
        """
        gamma_beta = self.gamma_beta_generator(cond_input)
        gamma = gamma_beta[:, :self.num_features]
        beta = gamma_beta[:, self.num_features:]
        return gamma, beta
