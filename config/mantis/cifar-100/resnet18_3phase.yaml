# config/mantis/cifar-100/resnet18_3phase.yaml
datasets:
  cifar100_original: # For pretraining teacher if needed, or general use
    type: 'CIFAR100'
    dataset_id: 'cifar100_original'
    params:
      root: '~/resources/datasets/cifar100'
      train: True
      download: True
    transform_params: # Using torchdistill's default transform building for simplicity here
      train:
        - type: 'RandomCrop'
          params:
            size: 32
            padding: 4
        - type: 'RandomHorizontalFlip'
        - type: 'ToTensor'
        - type: 'Normalize'
          params:
            mean: [0.5071, 0.4867, 0.4408]
            std: [0.2675, 0.2565, 0.2761]
      val:
        - type: 'ToTensor'
        - type: 'Normalize'
          params:
            mean: [0.5071, 0.4867, 0.4408]
            std: [0.2675, 0.2565, 0.2761]

  cifar100_5tasks_chunked:
    type: 'LabelChunkedTaskDataset' # Your custom wrapper
    name: 'cifar100_5tasks_chunked'
    splits:
      train:
        dataset_id: 'cifar100_5tasks_chunked/train'
        params:
          original_dataset: # This will be an instance of cifar100_original
            type: 'CIFAR100'
            params:
              root: '~/resources/datasets/cifar100'
              train: True
              download: True
              # Transform for the underlying CIFAR100. LabelChunkedTaskDataset might apply its own on top if needed.
              # If transforms are defined here, they apply before chunking logic.
              # Typically, the transforms from cifar100_original above would be reused by referring to it,
              # or defined explicitly if different for the chunked version's base.
              # For simplicity, let's assume LabelChunkedTaskDataset expects raw PIL images and labels,
              # and the final transforms are applied by the data loader config or a wrapper.
              # However, torchdistill usually expects dataset objects to have transforms embedded.
              # Let's assume 'cifar100_original/train' will be retrieved with its transforms.
          task_configs:
            - task_id: 0
              original_labels: { range: [0, 20] } # 20 classes per task
            - task_id: 1
              original_labels: { range: [20, 40] }
            - task_id: 2
              original_labels: { range: [40, 60] }
            - task_id: 3
              original_labels: { range: [60, 80] }
            - task_id: 4
              original_labels: { range: [80, 100] }
          default_task_id: -1
          default_task_specific_label: -1
          # image_key and label_key are not needed if original_dataset yields (img, label) tuples directly.
          # CIFAR100 from torchvision yields tuples.

      val:
        dataset_id: 'cifar100_5tasks_chunked/val'
        params:
          original_dataset:
            type: 'CIFAR100'
            params:
              root: '~/resources/datasets/cifar100'
              train: False # Validation set
              download: True
          task_configs:
            - task_id: 0
              original_labels: { range: [0, 20] }
            - task_id: 1
              original_labels: { range: [20, 40] }
            - task_id: 2
              original_labels: { range: [40, 60] }
            - task_id: 3
              original_labels: { range: [60, 80] }
            - task_id: 4
              original_labels: { range: [80, 100] }
          default_task_id: -1
          default_task_specific_label: -1
    transform_params: # Transforms applied *after* LabelChunkedTaskDataset yields (img, (main_label, task_target))
      train:
        - type: 'RandomCrop'
          params:
            size: 32
            padding: 4
        - type: 'RandomHorizontalFlip'
        - type: 'ToTensor'
        - type: 'Normalize'
          params:
            mean: [0.5071, 0.4867, 0.4408]
            std: [0.2675, 0.2565, 0.2761]
      val:
        - type: 'ToTensor'
        - type: 'Normalize'
          params:
            mean: [0.5071, 0.4867, 0.4408]
            std: [0.2675, 0.2565, 0.2761]

models:
  lmbda: 0.05
  distortion_metric_name: 'MSELoss' # General purpose, may not be directly used in all phases

  student_model:
    name: 'splittable_network_with_compressor_with_shared_stem'
    params:
      network_type: "FiLMedNetworkWithSharedStem" # Must match a registered class or func

      shared_stem_config:
        name: "SharedInputStem" # Not used by get_model, but for clarity
        params:
          in_channels: 3
          initial_out_channels: 32
          num_stem_layers: 2 # e.g. initial_conv -> layer1_conv. Total 2 downsamples by stride 2.
          final_stem_channels: 64 # Output channels of the last stem layer

      task_probability_model_config:
        name: "TaskProbabilityModel"
        params:
          # input_channels_from_stem: 64 (will be set dynamically from shared_stem_config.final_stem_channels)
          # output_cond_signal_dim will be set dynamically based on num_distinct_task_chunks_for_predictor from dataset
          # Example: if 5 distinct tasks, this might be 5 for one-hot, or a higher dimension for learned embeddings
          output_cond_signal_dim: 5 # Placeholder, will be overridden. For CIFAR-100/5 tasks, a 5-dim output could represent task logits.
          hidden_dims: [64, 32]
          dropout_rate: 0.1

      compression_module_config:
        name: "FiLMedHFactorizedPriorCompressionModule"
        params:
          entropy_bottleneck_channels: 96 # Number of channels for the main entropy bottleneck (y)
          analysis_config:
            name: "TaskConditionedFiLMedAnalysisNetwork"
            params:
              # input_channels_from_stem: 64 (will be set dynamically)
              latent_channels: 96 # Output of g_a, input to entropy_bottleneck
              block_configs: # Example: 2 blocks, each downsamples by 2
                - {out_channels: 128, kernel_size: 3, stride: 2, padding: 1, apply_film: True}
                - {out_channels: 96,  kernel_size: 3, stride: 1, padding: 1, apply_film: True} # Or stride 2 if more downsampling needed
              # film_cond_dim: 5 (will be set dynamically from task_probability_model output)
              film_generator_hidden_dim: 64
          synthesis_config:
            name: "SimpleSynthesisNetwork" # Example, replace with actual
            params:
              in_channels: 96 # Should match analysis_config.latent_channels (output of g_a)
              target_channels: 64 # To match backbone's expected input channels if its stem is skipped
              # channels: [96, 128, 64] # Example structure for SimpleSynthesisNetwork
          quantization_config: # For entropy bottleneck, if applicable
            backend: 'fbgemm'
            quant_device: 'cpu'

      backbone_config:
        name: get_timm_model
        params:
          timm_model_name: "resnet18"
          pretrained: false # Backbone weights are typically learned or fine-tuned
          no_classes: 20 # Number of classes per task chunk for CIFAR-100/5
          skip_embed: true # Important: ResNet's own stem (conv1, bn1, relu, maxpool) will be skipped
          split_idx: 1 # Start from layer1 of ResNet, as stem is handled by shared_stem

      reconstruction_layer_for_backbone_config: # Adapts g_s output to backbone input
        name: "ProjectionReconLayer" # Or ConvBlockReconLayer, or null if g_s output matches
        params:
          in_channels: 64 # Output of g_s's main path (before its internal final_layer)
          target_channels: 64 # Input channels expected by resnet18.layer1 (if backbone's own stem is skipped)
          # stride/upsample as needed, typically 1 for projection

      analysis_config_parent: # For AnalyzableModule, related to bpp/filesize analysis
        analyze_after_compress: True
        analyzers_config:
          - type: 'DiskFileSizeAndBppAnalyzer'
            params:
              unit: 'KB'

  teacher_model_phase1: # Teacher for Phase 1 distillation
    name: 'get_timm_model'
    params:
      timm_model_name: 'resnet18'
      pretrained: true
      no_classes: 100 # Original CIFAR-100 classes
      features_only: False # Ensure we can access intermediate layers by name. Or set to True if only features needed.

train:
  log_freq: 50

  phase1:
    num_epochs: 20
    trainable_modules:
      - "shared_stem"
      - "compression_module.g_a" # Trains all parts of g_a, including its film_generators
    eval_metrics: ['mse_loss_metric'] # Metric for distillation loss; to be defined if not standard
    train_data_loader:
      dataset_id: 'cifar100_original/train' # Use original CIFAR-100 for pretraining stem/encoder
      random_sample: True
      batch_size: 128
      num_workers: 4
      pin_memory: True
    val_data_loader:
      dataset_id: 'cifar100_original/val'
      random_sample: False
      batch_size: 256
      num_workers: 4
      pin_memory: True
    optimizer:
      type: 'AdamW'
      params:
        lr: 0.001
        weight_decay: 0.01
    scheduler:
      type: 'CosineAnnealingLR'
      params:
        T_max: 20 # Match num_epochs for phase1
        eta_min: 0.0
    criterion:
      type: 'GeneralizedCustomLoss'
      org_term: # Distillation Loss
        criterion:
          type: 'MSELoss'
          params: {}
        params:
          # Student's g_a output. Hook path is relative to student_model.
          input: {is_from_teacher: False, module_path: 'compression_module.g_a', io: 'output'}
          # Teacher's intermediate feature output. Hook path is relative to teacher_model.
          # For ResNet18, 'layer1' output might be a suitable target after shared_stem.
          # Adjust 'layer1' to match an actual named module in your teacher model that produces features
          # compatible (or made compatible via projection) with student's g_a output.
          target: {is_from_teacher: True, module_path: 'layer1', io: 'output'}
        factor: 1.0
    student: # Student model hooks for GeneralizedCustomLoss
      forward_hook:
        compression_module.g_a: {io: 'output'}
    teacher: # Teacher model hooks for GeneralizedCustomLoss
      forward_hook:
        layer1: {io: 'output'} # Must match a named module in teacher_model (e.g., resnet18.layer1)

  phase2:
    num_epochs: 30
    epoch_to_update_entropy_bottleneck: 5 # For self.student_model.update() if applicable
    trainable_modules:
      - "task_probability_model"
      - "compression_module.g_a.film_generators" # Only the FiLM parameter generators within g_a
      # Potentially compression_module.entropy_bottleneck if it has learnable params not updated by .update()
      # Usually entropy_bottleneck is updated via model.update() if it's from compressai.
    eval_metrics: [ 'accuracy', 'bpp', 'task_predictor_loss_metric', 'task0_accuracy', 'task1_accuracy', 'task2_accuracy', 'task3_accuracy', 'task4_accuracy' ]
    train_data_loader:
      dataset_id: 'cifar100_5tasks_chunked/train'
      random_sample: True
      batch_size: 128
      num_workers: 4
      pin_memory: True
    val_data_loader:
      dataset_id: 'cifar100_5tasks_chunked/val'
      random_sample: False
      batch_size: 256
      num_workers: 4
      pin_memory: True
    optimizer:
      type: 'AdamW'
      params:
        lr: 0.0005
        weight_decay: 0.01
    scheduler:
      type: 'CosineAnnealingLR'
      params:
        T_max: 30
        eta_min: 0.0
    criterion:
      type: 'GeneralizedCustomLoss'
      org_term: # Main task CrossEntropy loss
        criterion:
          type: 'CrossEntropyLoss'
          params: {}
        params:
          input: {is_from_teacher: False, module_path: '', io: 'output.main_output'}
          target: {is_from_teacher: False, module_path: '', io: 'target[0]'} # Assumes dataset returns (img, (main_target, task_target_for_predictor), ...)
        factor: 1.0
      sub_terms:
        rate_loss:
          criterion:
            type: 'BppLossOrig'
            params:
              input_sizes: [32, 32] # CIFAR-100 input resolution
              entropy_module_path: 'compression_module.entropy_bottleneck' # Path for hook
              reduction: 'mean'
          # params for BppLossOrig are implicitly handled by its forward signature (student_io_dict)
          factor: 0.05 # Corresponds to lmbda from top of YAML
        task_detector_loss:
          criterion:
            type: 'MultiLabelTaskRelevancyBCELoss' # Or nn.BCEWithLogitsLoss if TaskProbabilityModel outputs logits
            params:
              reduction: 'mean'
          params:
            input: {is_from_teacher: False, module_path: '', io: 'output.conditioning_signal_preview'} # Output of TaskProbabilityModel
            target: {is_from_teacher: False, module_path: '', io: 'target[1]'} # Multi-label binary target for task predictor
          factor: 0.5
    student: # Student model hooks for GeneralizedCustomLoss
      forward_hook:
        '': {io: 'output'} # Hook the entire student_model's output dictionary
        compression_module.entropy_bottleneck: {io: 'output'} # For BppLossOrig

  phase3:
    num_epochs: 15
    epoch_to_update_entropy_bottleneck: 3 # For self.student_model.update() if applicable
    trainable_modules:
      - "compression_module.g_s"
      - "compression_module.entropy_bottleneck" # For fine-tuning quantiles or if it has other learnable params
      - "backbone"
      - "reconstruction_for_backbone" # If it's not nn.Identity and has params
    eval_metrics: [ 'accuracy', 'bpp', 'task_predictor_loss_metric', 'task0_accuracy', 'task1_accuracy', 'task2_accuracy', 'task3_accuracy', 'task4_accuracy' ]
    train_data_loader: # Can be same as phase 2
      dataset_id: 'cifar100_5tasks_chunked/train'
      random_sample: True
      batch_size: 128
      num_workers: 4
      pin_memory: True
    val_data_loader:
      dataset_id: 'cifar100_5tasks_chunked/val'
      random_sample: False
      batch_size: 256
      num_workers: 4
      pin_memory: True
    optimizer:
      type: 'AdamW'
      params:
        lr: 0.0001 # Smaller LR for fine-tuning
        weight_decay: 0.01
    scheduler:
      type: 'CosineAnnealingLR'
      params:
        T_max: 15
        eta_min: 0.0
    criterion: # Same structure as phase 2, factors might change
      type: 'GeneralizedCustomLoss'
      org_term: { criterion: {type: 'CrossEntropyLoss', params: {}}, params: {input: {is_from_teacher: False, module_path: '', io: 'output.main_output'}, target: {is_from_teacher: False, module_path: '', io: 'target[0]'}}, factor: 1.0 }
      sub_terms:
        rate_loss: { criterion: {type: 'BppLossOrig', params: {input_sizes: [32, 32], entropy_module_path: 'compression_module.entropy_bottleneck', reduction: 'mean'}}, factor: 0.05 }
        task_detector_loss: { criterion: {type: 'MultiLabelTaskRelevancyBCELoss', params: {reduction: 'mean'}}, params: {input: {is_from_teacher: False, module_path: '', io: 'output.conditioning_signal_preview'}, target: {is_from_teacher: False, module_path: '', io: 'target[1]'}}, factor: 0.1 } # Maybe reduce factor if task_probability_model is frozen
    student: # Hooks remain the same as phase 2
      forward_hook:
        '': {io: 'output'}
        compression_module.entropy_bottleneck: {io: 'output'}

test:
  eval_metrics: [ 'accuracy', 'bpp_estimated', 'filesize_kb', 'task_predictor_accuracy' ]
  test_data_loader:
    dataset_id: 'cifar100_5tasks_chunked/val' # Or a dedicated test set
    random_sample: False
    batch_size: 256
    num_workers: 4