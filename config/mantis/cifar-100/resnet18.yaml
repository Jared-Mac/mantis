# config/mantis/cifar-100/resnet18.yaml
# ... (datasets, models sections remain largely the same) ...

models:
  lmbda: 0.05 
  distortion_metric_name: 'MSELoss' 

  student_model:
    name: 'splittable_network_with_compressor_with_shared_stem' 
    params:
      network_type: "FiLMedNetworkWithSharedStem" 

      shared_stem_config:
        name: "SharedInputStem" 
        params:
          in_channels: 3
          initial_out_channels: 32  
          num_stem_layers: 2        
          final_stem_channels: 64  

      task_probability_model_config:
        name: "TaskProbabilityModel" 
        params:
          # input_channels_from_stem: 64 (will be set dynamically)
          output_cond_signal_dim: 5   # Number of task chunks (0-4 for CIFAR-100/5 tasks)
          hidden_dims: [64, 32]       
          dropout_rate: 0.1

      compression_module_config:
        name: "FiLMedHFactorizedPriorCompressionModule" # Or other FiLMed compressor
        params:
          entropy_bottleneck_channels: 96 
          analysis_config: 
            name: "TaskConditionedFiLMedAnalysisNetwork"
            params:
              # input_channels_from_stem: 64 (will be set dynamically)
              latent_channels: 96 # Output of g_a
              block_configs: 
                - {out_channels: 128, kernel_size: 3, stride: 2, padding: 1, apply_film: True}
                - {out_channels: 96,  kernel_size: 3, stride: 1, padding: 1, apply_film: True}
              # film_cond_dim: 5 (will be set dynamically from task_probability_model_config)
              film_generator_hidden_dim: 64 
          synthesis_config: 
            name: "SimpleSynthesisNetwork" 
            params:
              in_channels: 96 
              target_channels: 64 
              # channels: [96, 64, 64, 64] # Example for SimpleSynthesisNetwork
          quantization_config: 
            backend: 'fbgemm' 
            quant_device: 'cpu' 
            
      backbone_config:
        name: get_timm_model 
        params:
          timm_model_name: "resnet18" 
          pretrained: false 
          no_classes: 20 
          skip_embed: true 
          split_idx: 1     
                           
      reconstruction_layer_for_backbone_config: 
        name: null 
                    
      analysis_config_parent: 
        analyze_after_compress: True 
        analyzers_config:
          - type: 'DiskFileSizeAndBppAnalyzer' 
            params:
              unit: 'KB' 
  
  teacher_model: # For Phase 1 Distillation
    name: 'get_timm_model'
    params:
      timm_model_name: 'resnet18' 
      pretrained: true 
      no_classes: 100 # Original CIFAR-100 classes
      # Make sure teacher model is not split if we need its early layers
      features_only: False # Ensure we can access intermediate layers by name

train:
  log_freq: 50
  # epoch_to_update: 5 # This is for entropy bottleneck, might be relevant in phase2 or 3

  # Phase 1: Train shared_stem and compression_module.g_a (FiLMed encoder)
  # using feature distillation from a pretrained teacher.
  phase1:
    num_epochs: 20
    trainable_modules:
      - "shared_stem"
      - "compression_module.g_a" # Trains all parts of g_a, including its film_generators
    eval_metrics: ['mse_loss_metric'] # Metric for distillation loss
    train_data_loader:
      dataset_id: 'cifar100_5tasks_chunked/train' # Or a general dataset for pretraining stem/encoder
      random_sample: True
      batch_size: 128
      num_workers: 4
      pin_memory: True
    val_data_loader:
      dataset_id: 'cifar100_5tasks_chunked/val'
      random_sample: False
      batch_size: 256
      num_workers: 4
      pin_memory: True
    optimizer:
      type: 'AdamW'
      params:
        lr: 0.001
        weight_decay: 0.01
    scheduler:
      type: 'CosineAnnealingLR'
      params:
        T_max: 20 # Match num_epochs for phase1
        eta_min: 0.0
    criterion:
      type: 'GeneralizedCustomLoss'
      org_term: # Distillation Loss
        criterion:
          type: 'MSELoss' # Example: Mean Squared Error for feature distillation
          params: {}
        params:
          # Student's g_a output. Hook path is relative to student_model.
          input: {is_from_teacher: False, module_path: 'compression_module.g_a', io: 'output'}
          # Teacher's intermediate feature output. Hook path is relative to teacher_model.
          # Example: if teacher is resnet18, 'layer1' output might be a suitable target.
          # Adjust 'layer1' to the actual layer name in your teacher model that produces features
          # compatible (or made compatible via projection) with student's g_a output.
          target: {is_from_teacher: True, module_path: 'layer1', io: 'output'}
        factor: 1.0
    student: # Student model hooks for GeneralizedCustomLoss
      forward_hook:
        # Hook to get the output of student's g_a module
        compression_module.g_a:
          io: 'output'
          # path: 'compression_module.g_a' # Not needed if key is the path
    teacher: # Teacher model hooks for GeneralizedCustomLoss
      forward_hook:
        # Hook to get the output of teacher's 'layer1' (example)
        layer1: # This must match a named module in the teacher_model
          io: 'output'
          # path: 'layer1' # Not needed if key is the path

  # Phase 2: Freeze stem and encoder's main blocks. Train task_probability_model and g_a.film_generators.
  phase2:
    num_epochs: 30
    epoch_to_update_entropy_bottleneck: 5 # For self.student_model.update()
    trainable_modules:
      - "task_probability_model"
      - "compression_module.g_a.film_generators" # Only the FiLM parameter generators within g_a
      # Potentially also compression_module.entropy_bottleneck if learning quantiles, but usually updated via .update()
    eval_metrics: [ 'accuracy', 'bpp', 'task_predictor_loss_metric', 'task0_accuracy', 'task1_accuracy', 'task2_accuracy', 'task3_accuracy', 'task4_accuracy' ]
    train_data_loader:
      dataset_id: 'cifar100_5tasks_chunked/train'
      random_sample: True
      batch_size: 128
      num_workers: 4
      pin_memory: True
    val_data_loader:
      dataset_id: 'cifar100_5tasks_chunked/val'
      random_sample: False
      batch_size: 256
      num_workers: 4
      pin_memory: True
    optimizer:
      type: 'AdamW'
      params:
        lr: 0.0005 # Potentially a smaller LR for this phase
        weight_decay: 0.01
    scheduler:
      type: 'CosineAnnealingLR'
      params:
        T_max: 30 # Match num_epochs for phase2
        eta_min: 0.0
    criterion:
      type: 'GeneralizedCustomLoss'
      org_term: # Main task CrossEntropy loss
        criterion:
          type: 'CrossEntropyLoss'
          params: {}
        params:
          input: {is_from_teacher: False, module_path: '', io: 'output.main_output'}
          target: {is_from_teacher: False, module_path: '', io: 'target[0]'} # Assumes dataset returns (img, (main_target, task_target), ...)
        factor: 1.0
      sub_terms:
        rate_loss:
          criterion:
            type: 'BppLossOrig'
            params:
              input_sizes: [32, 32] # CIFAR-100 input resolution
              entropy_module_path: 'compression_module.entropy_bottleneck' # Path relative to student_model for hook
              reduction: 'mean'
          factor: 0.05 # Corresponds to lmbda in YAML
        task_detector_loss:
          criterion:
            type: 'MultiLabelTaskRelevancyBCELoss'
            params:
              reduction: 'mean'
          params:
            input: {is_from_teacher: False, module_path: '', io: 'output.conditioning_signal_preview'} # Output of TaskProbabilityModel
            target: {is_from_teacher: False, module_path: '', io: 'target[1]'} # Multi-label binary target for task predictor
          factor: 0.5
    student: # Student model hooks for GeneralizedCustomLoss
      forward_hook:
        # Hook for the entire student_model output dictionary
        '': # Empty string means the model itself
          io: 'output'
        # Hook for the entropy bottleneck module for BppLoss
        compression_module.entropy_bottleneck:
          io: 'output'
          # path: 'compression_module.entropy_bottleneck' # Not needed if key is the path

  # Phase 3: Fine-tune tail models (backbone, g_s, entropy_bottleneck)
  phase3:
    num_epochs: 15
    epoch_to_update_entropy_bottleneck: 3
    trainable_modules:
      - "compression_module.g_s"
      - "compression_module.entropy_bottleneck" # For fine-tuning quantiles or if it has learnable params
      - "backbone"
      - "reconstruction_for_backbone" # If it's not nn.Identity and has params
    eval_metrics: [ 'accuracy', 'bpp', 'task_predictor_loss_metric', 'task0_accuracy', 'task1_accuracy', 'task2_accuracy', 'task3_accuracy', 'task4_accuracy' ]
    # ... (optimizer, scheduler, criterion similar to phase2, dataloaders) ...
    # Optimizer might need different LR for different parts (e.g., smaller LR for backbone)
    optimizer:
      type: 'AdamW'
      params:
        lr: 0.0001 # Smaller LR for fine-tuning
        weight_decay: 0.01
    scheduler:
      type: 'CosineAnnealingLR'
      params:
        T_max: 15 # Match num_epochs for phase3
        eta_min: 0.0
    criterion: # Same as phase 2, but factors might change
      type: 'GeneralizedCustomLoss'
      org_term: { criterion: {type: 'CrossEntropyLoss', params: {}}, params: {input: {is_from_teacher: False, module_path: '', io: 'output.main_output'}, target: {is_from_teacher: False, module_path: '', io: 'target[0]'}}, factor: 1.0 }
      sub_terms:
        rate_loss: { criterion: {type: 'BppLossOrig', params: {input_sizes: [32, 32], entropy_module_path: 'compression_module.entropy_bottleneck', reduction: 'mean'}}, factor: 0.05 }
        task_detector_loss: { criterion: {type: 'MultiLabelTaskRelevancyBCELoss', params: {reduction: 'mean'}}, params: {input: {is_from_teacher: False, module_path: '', io: 'output.conditioning_signal_preview'}, target: {is_from_teacher: False, module_path: '', io: 'target[1]'}}, factor: 0.1 } # Maybe reduce task detector loss factor
    student: # Hooks remain the same as phase 2
      forward_hook:
        '': {io: 'output'}
        compression_module.entropy_bottleneck: {io: 'output'}
    # DataLoaders can be the same as phase 2
    train_data_loader:
      dataset_id: 'cifar100_5tasks_chunked/train'
      random_sample: True
      batch_size: 128
      num_workers: 4
      pin_memory: True
    val_data_loader:
      dataset_id: 'cifar100_5tasks_chunked/val'
      random_sample: False
      batch_size: 256
      num_workers: 4
      pin_memory: True

test: # Test config remains the same
  eval_metrics: [ 'accuracy', 'bpp_estimated', 'filesize_kb', 'task_predictor_accuracy' ]
  test_data_loader:
    dataset_id: 'cifar100_5tasks_chunked/val'
    random_sample: False
    batch_size: 256
    num_workers: 4