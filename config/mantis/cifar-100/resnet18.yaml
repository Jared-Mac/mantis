# Fully Specified End-to-End YAML Configuration for CIFAR-100
# This configuration defines a multi-task learning setup with a shared input stem,
# a task probability model generating FiLM conditioning signals, and a FiLM-conditioned
# analysis network within the compression module.

datasets:
  # 1. Base CIFAR-100 Dataset Definition (referenced by the wrapper below)
  # This section defines how to load the raw CIFAR100 dataset.
  cifar100_base_definition: 
    name: 'cifar100'
    type: 'CIFAR100' 
    root: '~/data/cifar-100' # <<< ADJUST PATH HERE
    transform_params: 
      train:
        - type: 'RandomCrop'
          params:
            size: [ 32, 32 ] 
            padding: 4
        - type: 'RandomHorizontalFlip'
          params: {} 
        - type: 'ToTensor'
          params: {}
        - type: 'Normalize' 
          params:
            mean: [ 0.5071, 0.4867, 0.4408 ]
            std: [ 0.2675, 0.2565, 0.2761 ]
      val: 
        - type: 'ToTensor'
          params: {}
        - type: 'Normalize'
          params:
            mean: [ 0.5071, 0.4867, 0.4408 ]
            std: [ 0.2675, 0.2565, 0.2761 ]

  # 2. LabelChunkedTaskDataset for CIFAR-100
  cifar100_5tasks_chunked:
    name: 'cifar100_5tasks_chunked'
    type: 'LabelChunkedTaskDataset' 
    splits:
      train:
        dataset_id: 'cifar100_5tasks_chunked/train'
        params: 
          original_dataset:
            type: 'CIFAR100'
            params:
              root: '~/data/cifar100' # <<< ADJUST PATH HERE
              train: True
              download: True
              transform_params: # This list will be parsed into a Compose object
                - type: 'RandomCrop'
                  params:
                    size: [ 32, 32 ]
                    padding: 4
                - type: 'RandomHorizontalFlip'
                  params: {}
                - type: 'ToTensor'
                  params: {}
                - type: 'Normalize'
                  params:
                    mean: [ 0.5071, 0.4867, 0.4408 ]
                    std: [ 0.2675, 0.2565, 0.2761 ]
          task_configs: 
            - task_id: 0 
              original_labels: { range: [0, 20] }  # CIFAR-100 classes 0-19
            - task_id: 1
              original_labels: { range: [20, 40] } # CIFAR-100 classes 20-39
            - task_id: 2
              original_labels: { range: [40, 60] } # CIFAR-100 classes 40-59
            - task_id: 3
              original_labels: { range: [60, 80] } # CIFAR-100 classes 60-79
            - task_id: 4
              original_labels: { range: [80, 100] }# CIFAR-100 classes 80-99
          default_task_id: -1      
          default_task_specific_label: -1   
      val:
        dataset_id: 'cifar100_5tasks_chunked/val'
        params: 
          original_dataset:
            type: 'CIFAR100'
            params:
              root: '~/data/cifar100' # <<< ADJUST PATH HERE
              train: False 
              download: True
              transform:
                _target_: "your_transform_parser_function_or_torchdistill_default" # Placeholder
                config: # List of transform dicts, matching cifar100_base_definition.transform_params.val
                  - type: 'ToTensor'
                    params: {}
                  - type: 'Normalize'
                    params:
                      mean: [ 0.5071, 0.4867, 0.4408 ]
                      std: [ 0.2675, 0.2565, 0.2761 ]
          task_configs: 
            - task_id: 0 
              original_labels: { range: [0, 20] }
            - task_id: 1
              original_labels: { range: [20, 40] }
            - task_id: 2
              original_labels: { range: [40, 60] }
            - task_id: 3
              original_labels: { range: [60, 80] }
            - task_id: 4
              original_labels: { range: [80, 100] }
          default_task_id: -1
          default_task_specific_label: -1

models:
  lmbda: 0.05 
  distortion_metric_name: 'MSELoss' 

  student_model:
    name: 'splittable_network_with_compressor_with_shared_stem' 
    params:
      network_type: "FiLMedNetworkWithSharedStem" 

      shared_stem_config:
        name: "SharedInputStem" 
        params:
          in_channels: 3
          initial_out_channels: 32  
          num_stem_layers: 2        
          final_stem_channels: 64  

      task_probability_model_config:
        name: "TaskProbabilityModel" 
        params:
          # input_channels_from_stem: 64 (will be set dynamically)
          output_cond_signal_dim: 5   # Number of task chunks (0-4 for CIFAR-100/5 tasks)
          hidden_dims: [64, 32]       
          dropout_rate: 0.1

      compression_module_config:
        name: "FiLMedHFactorizedPriorCompressionModule" 
        params:
          entropy_bottleneck_channels: 96 
          analysis_config: 
            name: "TaskConditionedFiLMedAnalysisNetwork"
            params:
              # input_channels_from_stem: 64 (will be set dynamically)
              latent_channels: 96 # Output of g_a
              block_configs: 
                # Input to g_a is (B, 64, 8, 8) from stem (assuming 2 stem layers with stride 2 from 32x32)
                - {out_channels: 128, kernel_size: 3, stride: 2, padding: 1, apply_film: True}  # 64ch,8x8 -> 128ch,4x4
                - {out_channels: 96,  kernel_size: 3, stride: 1, padding: 1, apply_film: True}  # 128ch,4x4 -> 96ch,4x4 (latent_dim)
              # film_cond_dim: 5 (will be set dynamically from task_probability_model_config)
              film_generator_hidden_dim: 64 
          synthesis_config: 
            name: "SimpleSynthesisNetwork" 
            params:
              in_channels: 96 # Matches analysis_config.latent_channels
              target_channels: 64 # Output channels for the backbone (e.g. ResNet18 layer1)
              # For SimpleSynthesisNetwork, example channel progression for latent (96ch, 4x4) -> output (64ch, 8x8 for ResNet18)
              # This requires one upsampling stage.
              # SimpleSynthesisNetwork: rb1(upsample), rb2(no_upsample), rb3(no_upsample)
              # Channels: [96, 64, 64, 64] -> output is 64 channels at 8x8
              # channels: [96, 64, 64, 64] 
          quantization_config: 
            backend: 'fbgemm' 
            quant_device: 'cpu' 
            
      backbone_config:
        name: get_timm_model 
        params:
          timm_model_name: "resnet18" 
          pretrained: false # Set to true if using ImageNet pretrained weights and fine-tuning
          no_classes: 20 # Number of classes *within each task chunk* (100 classes / 5 tasks = 20)
          skip_embed: true 
          split_idx: 1     # Start ResNet from layer1 (expects 64 channels)
                           
      reconstruction_layer_for_backbone_config: 
        name: null # Assuming synthesis_config.target_channels (64) directly matches backbone's input needs.
        # params: {} # Ignored if name is null.
                    
      analysis_config_parent: 
        analyze_after_compress: True 
        analyzers_config:
          - type: 'DiskFileSizeAndBppAnalyzer' 
            params:
              unit: 'KB' 
  
  teacher_model: # Optional
    name: 'get_timm_model'
    params:
      timm_model_name: 'resnet18' 
      pretrained: true 
      no_classes: 100 # Or 20 if task-specific teacher

train:
  log_freq: 50 
  epoch_to_update: 5 

  stage1: 
    eval_metrics: [ 'accuracy', 'bpp', 'task_predictor_loss_metric', 'task0_accuracy', 'task1_accuracy', 'task2_accuracy', 'task3_accuracy', 'task4_accuracy' ] 
    num_epochs: 30 
    
    train_data_loader:
      dataset_id: 'cifar100_5tasks_chunked/train' 
      random_sample: True
      batch_size: 128 
      num_workers: 4  
      pin_memory: True
      
    val_data_loader:
      dataset_id: 'cifar100_5tasks_chunked/val' 
      random_sample: False
      batch_size: 256 
      num_workers: 4
      pin_memory: True
    
    student: 
      forward_hook:
        output: 
          - '' 
          - 'compression_module.entropy_bottleneck'
      requires_grad: True

    optimizer:
      type: 'AdamW'
      params:
        lr: 0.001 
        weight_decay: 0.01
    
    scheduler:
      type: 'CosineAnnealingLR'
      params:
        T_max: 30 
        eta_min: 0.0

    criterion:
      type: 'GeneralizedCustomLoss' 
      org_term: 
        criterion:
          type: 'CrossEntropyLoss'
          params: {}
        params: 
          input: {is_from_teacher: False, module_path: '', io: 'output.main_output'} 
          target: {is_from_teacher: False, module_path: '', io: 'target[0]'} 
        factor: 1.0 
      
      sub_terms:
        rate_loss: 
          criterion:
            type: 'BppLossOrig' 
            params:
              input_sizes: [32, 32] # CIFAR-100 input resolution
              entropy_module_path: 'compression_module.entropy_bottleneck'
              reduction: 'mean' 
          factor: 0.05 # Explicitly set from lmbda_cifar
        
        task_detector_loss: 
          criterion:
            type: 'MultiLabelTaskRelevancyBCELoss' 
            params:
              reduction: 'mean' 
          params:
            input: {is_from_teacher: False, module_path: '', io: 'output.conditioning_signal_preview'}
            target: {is_from_teacher: False, module_path: '', io: 'target[1]'} 
          factor: 0.5 

test:
  eval_metrics: [ 'accuracy', 'bpp_estimated', 'filesize_kb', 'task_predictor_accuracy' ] 
  test_data_loader:
    dataset_id: 'cifar100_5tasks_chunked/val' 
    random_sample: False
    batch_size: 256 
    num_workers: 4
